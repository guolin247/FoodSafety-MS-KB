import json
import os
import pandas as pd
from jsonschema import validate, exceptions
from datetime import datetime

# ================= CONFIGURATION =================
# Detections æ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹ (è¯·ç¡®è®¤è·¯å¾„æ­£ç¡®)
DATA_FOLDER = r"D:\work_GuoLin\FoodSafety-MS-KB\validation_scripts\detections"
# Schema æ–‡ä»¶è·¯å¾„ (è¯·ç¡®è®¤è·¯å¾„æ­£ç¡®)
SCHEMA_PATH = r"D:\work_GuoLin\FoodSafety-MS-KB\validation_scripts\schema.json"
# è¾“å‡ºæ—¥å¿—æ–‡ä»¶
OUTPUT_LOG = "Table_S3_Detections_Diagnostic_Log.xlsx"
# ============================================

# Detections åœ¨ Schema ä¸­çš„å®šä¹‰è·¯å¾„
DETECTION_DEFINITION_PATH = ["definitions", "detections"]

def load_json_file(file_path):
    """åŠ è½½ JSON æ–‡ä»¶å†…å®¹"""
    if not os.path.exists(file_path):
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return None

def get_detection_schema_content(schema):
    """æå– detections çš„åŽŸå§‹å±žæ€§å®šä¹‰"""
    current_def = schema
    try:
        for key in DETECTION_DEFINITION_PATH:
            current_def = current_def[key]
        return current_def
    except KeyError:
        return None

def get_item_validation_schema(schema):
    """æž„å»ºå•ä¸ª Detection Item çš„éªŒè¯ Schema (å« AnyOf è§„åˆ™ï¼Œä¸”å…è®¸ Null)"""
    detection_properties = get_detection_schema_content(schema)
    if detection_properties is None: return None
    
    # æµ…æ‹·è´ properties
    modified_properties = detection_properties.copy()
    
    # 1. å…è®¸ CAS_number ä¸º null
    if "CAS_number" in modified_properties:
        orig = modified_properties["CAS_number"].copy()
        if orig.get("type") == "string": orig["type"] = ["string", "null"]
        modified_properties["CAS_number"] = orig

    # 2. å…è®¸ compound_english_name ä¸º null
    if "compound_english_name" in modified_properties:
        orig = modified_properties["compound_english_name"].copy()
        if orig.get("type") == "string": orig["type"] = ["string", "null"]
        modified_properties["compound_english_name"] = orig

    # 3. [NEW] å…è®¸ mass_spec_params é‡Œçš„ collision_energy ä¸º null
    # è¿™æ¯”è¾ƒæ·±ï¼Œéœ€è¦è¿›åˆ° items -> properties
    if "mass_spec_params" in modified_properties:
        msp = modified_properties["mass_spec_params"].copy()
        if "items" in msp:
            msp_items = msp["items"].copy()
            if "properties" in msp_items:
                msp_props = msp_items["properties"].copy()
                if "collision_energy" in msp_props:
                    ce_def = msp_props["collision_energy"].copy()
                    # æ”¾å®½ç±»åž‹ï¼šå…è®¸ object æˆ– null
                    if ce_def.get("type") == "object":
                        ce_def["type"] = ["object", "null"]
                    msp_props["collision_energy"] = ce_def
                msp_items["properties"] = msp_props
            msp["items"] = msp_items
        modified_properties["mass_spec_params"] = msp

    return {
        "type": "object",
        "properties": modified_properties,
        "anyOf": [
            # è¿™é‡Œçš„é€»è¾‘ä¿æŒä¸å˜ï¼šè¦æ±‚è‡³å°‘æœ‰ä¸€ä¸ªå­—æ®µæ˜¯éžç©ºçš„ String
            {"properties": {"CAS_number": {"type": "string"}}, "required": ["CAS_number"]},
            {"properties": {"compound_english_name": {"type": "string"}}, "required": ["compound_english_name"]}
        ]
    }

def check_key_presence(data, schema_properties, path=""):
    """
    é€’å½’æ£€æŸ¥é”®å®Œæ•´æ€§ã€‚
    å…¼å®¹æ€§é€»è¾‘ï¼šåªè¦é”®å­˜åœ¨ï¼Œå€¼ä¸º None ä¹Ÿè§†ä¸ºé€šè¿‡ã€‚
    """
    missing_keys = []
    
    for key, definition in schema_properties.items():
        current_path = f"{path}.{key}" if path else key
        
        # 1. æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨
        if key not in data:
            missing_keys.append(current_path)
            continue
        
        # 2. å¦‚æžœé”®å­˜åœ¨ä½†å€¼ä¸º Noneï¼Œç›´æŽ¥è§†ä¸ºé€šè¿‡ï¼Œä¸æ£€æŸ¥å­ç»“æž„
        if data[key] is None:
            continue

        # 3. å¦‚æžœæ˜¯å¯¹è±¡ï¼Œé€’å½’æ£€æŸ¥
        if definition.get("type") == "object":
            if "properties" in definition and isinstance(data[key], dict):
                missing_keys.extend(
                    check_key_presence(data[key], definition["properties"], current_path)
                )
            
        # 4. å¦‚æžœæ˜¯æ•°ç»„ï¼ŒéåŽ†æ£€æŸ¥æ¯ä¸ªå…ƒç´ 
        elif definition.get("type") == "array" and "items" in definition:
            item_schema = definition.get("items", {})
            if item_schema.get("type") == "object" and "properties" in item_schema and isinstance(data[key], list):
                for i, item_data in enumerate(data[key]):
                    item_path = f"{current_path}[{i}]" 
                    if isinstance(item_data, dict):
                        missing_keys.extend(
                            check_key_presence(item_data, item_schema["properties"], item_path)
                        )
    
    return missing_keys

def run_batch_validation():
    print(f"ðŸš€ Starting DIAGNOSTIC Detections Validation...")
    print(f"ðŸ“‚ Scanning: {DATA_FOLDER}")
    
    # 1. Load Schema
    schema_content = load_json_file(SCHEMA_PATH)
    if not schema_content:
        print("âŒ Critical Error: Cannot load schema.")
        return

    schema_props = get_detection_schema_content(schema_content)
    item_validator = get_item_validation_schema(schema_content)
    
    if not schema_props or not item_validator:
        print("âŒ Critical Error: Invalid schema structure.")
        return

    # 2. Iterate Files
    results = []
    files = [f for f in os.listdir(DATA_FOLDER) if f.endswith(('.json', '.txt'))]
    print(f"ðŸ“Š Found {len(files)} files to process.")

    for filename in files:
        file_path = os.path.join(DATA_FOLDER, filename)
        
        # Load Data
        data = load_json_file(file_path)
        if data is None:
            results.append({"File Name": filename, "Status": "Load Error"})
            continue
            
        # --- 1. è‡ªé€‚åº”æ•°æ®ç»“æž„ (Auto-unwrap) ---
        target_list = []
        is_unwrapped = False
        
        if isinstance(data, list):
            target_list = data
        elif isinstance(data, dict):
            # å°è¯•æ‰¾ detections é”®
            if "detections" in data and isinstance(data["detections"], list):
                target_list = data["detections"]
                is_unwrapped = True
            else:
                # å°è¯•å°†æ•´ä¸ªå­—å…¸ä½œä¸ºå•æ¡è®°å½•
                target_list = [data]
                is_unwrapped = True # æ ‡è®°ä¸ºç»è¿‡äº†å¤„ç†
        
        # --- 2. é€æ¡éªŒè¯ ---
        file_errors = []
        
        for idx, item in enumerate(target_list):
            # Schema Check
            try:
                validate(instance=item, schema=item_validator)
            except exceptions.ValidationError as e:
                # è®°å½•è¯¦ç»†é”™è¯¯è·¯å¾„
                path_str = ".".join(str(x) for x in e.path) if e.path else "root"
                file_errors.append(f"[Row {idx}] Schema: {e.message} @ {path_str}")
            
            # Completeness Check
            missing = check_key_presence(item, schema_props)
            if missing:
                file_errors.append(f"[Row {idx}] Missing Keys: {', '.join(missing[:3])}")

        # --- 3. ç»“æžœæ±‡æ€»ä¸Žè¾“å‡º ---
        is_pass = len(file_errors) == 0
        icon = "âœ…" if is_pass else "âŒ"
        
        # æž„å»ºçŠ¶æ€æè¿°
        status_msg = f"{filename}"
        if is_unwrapped:
            status_msg += " (Unwrapped)"
        
        print(f"  {icon} {status_msg} | Records: {len(target_list)}")
        
        if not is_pass:
            # æ‰“å°å‰3ä¸ªé”™è¯¯ä¾›è¯Šæ–­
            for err in file_errors[:3]:
                print(f"     -> {err}")
            if len(file_errors) > 3:
                print(f"     -> ... ({len(file_errors)-3} more errors)")

        results.append({
            "File Name": filename,
            "Record Count": len(target_list),
            "Status": "PASS" if is_pass else "FAIL",
            "Error Count": len(file_errors),
            "First 3 Errors": " || ".join(file_errors[:3]),
            "Timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })

    # 3. Save Report
    df = pd.DataFrame(results)
    df.to_excel(OUTPUT_LOG, index=False)
    print(f"\nðŸ’¾ Diagnostic Log saved to: {OUTPUT_LOG}")
    
    # Summary
    pass_count = len(df[df["Status"] == "PASS"])
    print(f"ðŸ“ˆ Summary: {pass_count}/{len(files)} files passed all checks.")

if __name__ == "__main__":
    run_batch_validation()